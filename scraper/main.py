import json
import re
import os
import time
from news_api import NewsEngine
from naver_api import NaverManager
from database import DatabaseManager
from supabase import create_client

# ì„¤ì •
TARGET_COUNTS = 10  # [ì„¤ì •] í•œë²ˆ ì‹¤í–‰ ë‹¹ ìµœëŒ€ ì‘ì„± ì¸ì› ìˆ˜

def clean_json_text(text):
    match = re.search(r"```(?:json)?\s*(.*)\s*```", text, re.DOTALL)
    if match: text = match.group(1)
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1: return text[start:end+1]
    return text.strip()

# ---------------------------------------------------------
# [Supabase ì—°ê²° ë° Run Count ê´€ë¦¬]
# ---------------------------------------------------------
supa_url = os.environ.get("SUPABASE_URL")
supa_key = os.environ.get("SUPABASE_KEY")
supabase = create_client(supa_url, supa_key) if supa_url and supa_key else None

def get_run_count():
    """
    ì´ë¯¸ì§€ì˜ system_status í…Œì´ë¸”ì—ì„œ í˜„ì¬ ì‹¤í–‰ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    if not supabase: return 0
    try:
        # idê°€ 1ì¸ rowì˜ run_countë¥¼ ê°€ì ¸ì˜´
        res = supabase.table('system_status').select('run_count').eq('id', 1).single().execute()
        return res.data['run_count'] if res.data else 0
    except:
        # í…Œì´ë¸”ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—ëŸ¬ë‚˜ë©´ 0ë¶€í„° ì‹œì‘
        return 0

def update_run_count(current):
    """
    ì‘ì—… ì™„ë£Œ í›„ ì‹¤í–‰ ë²ˆí˜¸ë¥¼ +1 ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ (0~23 ì‚¬ì´ ìˆœí™˜).
    """
    if not supabase: return
    next_count = current + 1
    if next_count >= 24: next_count = 0  # 24ê°€ ë˜ë©´ ë‹¤ì‹œ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    try:
        supabase.table('system_status').upsert({'id': 1, 'run_count': next_count}).execute()
        print(f"ğŸ”„ Cycle Count Updated: {current} -> {next_count}")
    except Exception as e:
        print(f"âš ï¸ Failed to update run count: {e}")

# ìˆœìœ„ ë³€ë™ ì²´í¬ìš© (ì´ì „ ë­í‚¹ ë¡œë”©)
def get_previous_rank_map(category):
    if not supabase: return {}
    try:
        res = supabase.table('search_archive') \
            .select('keyword, query') \
            .eq('category', category) \
            .order('created_at', desc=True) \
            .limit(100) \
            .execute()
        rank_map = {}
        if res.data:
            for item in res.data:
                kw = item['keyword']
                if kw in rank_map: continue
                try:
                    match = re.search(r'rank (\d+)', item['query'])
                    if match: rank_map[kw] = int(match.group(1))
                except: pass
        return rank_map
    except: return {}

# ---------------------------------------------------------
# [ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜]
# ---------------------------------------------------------
def run_automation():
    # 1. í˜„ì¬ ëª‡ ë²ˆì§¸ ì‹¤í–‰ì¸ì§€ í™•ì¸ (DBì—ì„œ ë¡œë“œ)
    run_count = get_run_count()
    print(f"ğŸš€ Automation Started (Cycle: {run_count}/23)")
    
    db = DatabaseManager()
    engine = NewsEngine(run_count)  # run_countì— ë”°ë¼ API í‚¤ë¥¼ ë°”ê¿”ì„œ ì—”ì§„ ì‹œì‘
    naver = NaverManager()
    
    # K-Popì˜ ê²½ìš°ì—ë§Œ ì°¨íŠ¸ë¥¼ ê°±ì‹ í• ì§€ ê²°ì • (ì˜µì…˜)
    is_key1 = engine.is_using_primary_key() 
    
    categories = ["k-pop", "k-drama", "k-movie", "k-entertain", "k-culture"]

    for cat in categories:
        print(f"\n[{cat}] Starting Analysis...")
        
        # [Phase 1] Top 10 ì°¨íŠ¸ ì¡°ì‚¬ ë° ì €ì¥
        should_update_chart = (cat == 'k-pop') or is_key1
        if should_update_chart:
            try:
                chart_json = engine.get_top10_chart(cat)
                cleaned_chart = clean_json_text(chart_json)
                if cleaned_chart and cleaned_chart != "{}":
                    parsed_chart = json.loads(cleaned_chart)
                    top10_list = parsed_chart.get('top10', [])
                    if top10_list:
                        print(f"  > ğŸ“Š Saving Top 10 Chart ({len(top10_list)} items)...")
                        db_data = []
                        for item in top10_list:
                            db_data.append({
                                "category": cat,
                                "rank": item.get('rank'),
                                "title": item.get('title'),
                                "meta_info": item.get('info', ''),
                                "score": item.get('score', 0)
                            })
                        db.save_rankings(db_data)
            except Exception as e:
                print(f"  > âŒ Phase 1 Error: {e}")

        # =========================================================
        # [Phase 2] Top 30 ì¸ë¬¼ ë‰´ìŠ¤ ì¡°ì‚¬ ë° í•„í„°ë§
        # =========================================================
        try:
            prev_ranks = get_previous_rank_map(cat)
            people_json = engine.get_top30_people(cat)
            cleaned_people = clean_json_text(people_json)
            
            if not cleaned_people or cleaned_people == "{}":
                continue
                
            parsed_people = json.loads(cleaned_people)
            people_list = parsed_people.get('people', [])
            
            if people_list:
                print(f"  > ğŸ‘¥ Analyzing {len(people_list)} Candidates...")
                live_news_buffer = []

                for person in people_list:
                    # [í•„í„° 1] ëª©í‘œ ì¸ì›(10ëª…) ì±„ì› ìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì¢…ë£Œ
                    if len(live_news_buffer) >= TARGET_COUNTS:
                        print("  > âœ… Target count (10) reached. Stopping loop.")
                        break

                    rank = person.get('rank')
                    name_en = person.get('name_en')
                    name_kr = person.get('name_kr')
                    if not name_kr: name_kr = name_en
                    
                    if not name_en or not rank: continue

                    # [í•„í„° 2] ì¿¨íƒ€ì„ ì²´í¬ (DB í™•ì¸ - ìµœê·¼ì— ì‘ì„±í–ˆìœ¼ë©´ íŒ¨ìŠ¤)
                    if engine.is_in_cooldown(name_en):
                        continue

                    # [ë¡œì§] ê¸°ì‚¬ ì‘ì„± ëŒ€ìƒ ì„ ì • (Top 3 or ìˆœìœ„ ë³€ë™ or ì‹ ê·œ ì§„ì…)
                    should_write = False
                    reason = ""
                    if rank <= 3:
                        should_write = True; reason = "ğŸ”¥ Top 3"
                    elif name_en not in prev_ranks:
                        should_write = True; reason = "âœ¨ New Entry"
                    elif prev_ranks.get(name_en) != rank:
                        should_write = True; reason = "ğŸ“ˆ Rank Change"
                    
                    if should_write:
                        print(f"    -> ğŸ“ Processing #{rank} {name_en} ({reason})...")
                        
                        # (A) ê¸°ì‚¬ íŒ©íŠ¸ ìˆ˜ì§‘ (ì—¬ê¸°ì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ìœ ë¬´ë„ ë‚´ë¶€ì ìœ¼ë¡œ ì²´í¬ë¨)
                        facts = engine.fetch_article_details(name_kr, name_en, cat, rank)
                        
                        # [í•„í„° 3] ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                        if "NO NEWS FOUND" in facts or "Failed" in facts:
                            continue

                        # (B) Groq AIë¡œ ê¸°ì‚¬ ì‘ì„±
                        full_text = engine.edit_with_groq(name_en, facts, cat)
                        
                        # (C) ì ìˆ˜ íŒŒì‹± (ê¸°ë³¸ 70ì )
                        score = 70
                        if "###SCORE:" in full_text:
                            try:
                                parts = full_text.split("###SCORE:")
                                full_text = parts[0].strip()
                                m = re.search(r'\d+', parts[1])
                                if m: score = int(m.group())
                            except: pass

                        # (D) ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬
                        lines = full_text.split('\n')
                        title = lines[0].replace('Headline:', '').strip()
                        summary = "\n".join(lines[1:]).strip()

                        # (E) DB ì €ì¥ìš© ë°ì´í„° ìƒì„±
                        # â˜…â˜…â˜… ì¤‘ìš”: ì—¬ê¸°ì„œ run_countë¥¼ ë„£ì§€ ì•ŠìŠµë‹ˆë‹¤ (live_news í…Œì´ë¸”ì— ì—†ìœ¼ë¯€ë¡œ) â˜…â˜…â˜…
                        article_data = {
                            "category": cat,
                            "keyword": name_en,
                            "title": title,
                            "summary": summary,
                            "image_url": naver.get_image(name_kr), # ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰
                            "score": score,
                            "likes": 0,
                            # "run_count": run_count  <-- [ì‚­ì œë¨] ì—ëŸ¬ ë°©ì§€ìš©
                        }
                        
                        # ì•„ì¹´ì´ë¸Œìš© ë°ì´í„° (ì—¬ê¸°ì—” run_countë‚˜ query ì •ë³´ë¥¼ ë„£ê³  ì‹¶ë‹¤ë©´ ë³„ë„ë¡œ êµ¬ì„± ê°€ëŠ¥)
                        archive_data = article_data.copy()
                        archive_data["query"] = f"{cat} top 30 rank {rank}"
                        # archive_data["run_count"] = run_count # search_archive í…Œì´ë¸”ì— ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì£¼ì„ í•´ì œ

                        # ì‹¤ì œ DB ì €ì¥ (ArchiveëŠ” 7ì¼ì¹˜ ë³´ê´€)
                        db.save_to_archive(archive_data)
                        
                        # ë¼ì´ë¸Œ ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ (LiveëŠ” ìµœì‹  50ê°œ ìœ ì§€)
                        live_news_buffer.append(article_data)
                        
                        # [ì¤‘ìš”] ì‘ì„± ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì¿¨íƒ€ì„ DBì— ê¸°ë¡! (ì¤‘ë³µ ì‘ì„± ë°©ì§€)
                        engine.update_history(name_en, cat)
                        
                        time.sleep(1)

                # í•œ ì¹´í…Œê³ ë¦¬ ë£¨í”„ê°€ ëë‚˜ë©´ ëª¨ì¸ ê¸°ì‚¬ë“¤ì„ í•œ ë²ˆì— Live Newsì— ì €ì¥
                if live_news_buffer:
                    print(f"  > ğŸ’¾ Saving {len(live_news_buffer)} articles to Live News...")
                    db.save_live_news(live_news_buffer)
                else:
                    print("  > ğŸ’¤ No valid news found this cycle.")

        except Exception as e:
            print(f"  > âŒ Phase 2 Error: {e}")

    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì‘ì—…ì´ ëë‚˜ë©´ Run Count ì—…ë°ì´íŠ¸ (+1)
    update_run_count(run_count)

if __name__ == "__main__":
    run_automation()
