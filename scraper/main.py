import os
import sys
import json
import urllib.request
import urllib.parse
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq
from urllib.parse import urljoin

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
supabase_key = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
groq_api_key = os.environ.get("GROQ_API_KEY")
naver_client_id = os.environ.get("NAVER_CLIENT_ID")
naver_client_secret = os.environ.get("NAVER_CLIENT_SECRET")

supabase: Client = create_client(supabase_url, supabase_key)
groq_client = Groq(api_key=groq_api_key)
AI_MODEL = "llama-3.3-70b-versatile"

SEARCH_KEYWORDS = ["K-POP ì•„ì´ëŒ", "í•œêµ­ ì¸ê¸° ë“œë¼ë§ˆ", "í•œêµ­ ì˜í™” í™”ì œ", "í•œêµ­ ì˜ˆëŠ¥ ë ˆì „ë“œ"]

def get_real_news_image(link):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Referer': 'https://news.naver.com/'
        }
        response = requests.get(link, headers=headers, timeout=10)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. og:image íƒìƒ‰
        og_image = soup.find('meta', property='og:image')
        img_url = og_image['content'] if og_image and og_image.get('content') else None
        
        # 2. ë³¸ë¬¸ ì´ë¯¸ì§€ íƒìƒ‰
        if not img_url or "static.naver.net" in img_url:
            selectors = ['#dic_area img', '#articleBodyContents img', '.article_kanvas img', '.article_body img']
            for s in selectors:
                tag = soup.select_one(s)
                if tag and tag.get('src'):
                    img_url = tag['src']
                    break
        
        return urljoin(link, img_url) if img_url else None
    except: return None

def get_naver_api_news(keyword):
    encText = urllib.parse.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/news?query={encText}&display=15&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", naver_client_id)
    req.add_header("X-Naver-Client-Secret", naver_client_secret)
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

def ai_chief_editor(news_batch):
    news_text = ""
    for idx, item in enumerate(news_batch):
        clean_title = item['title'].replace('<b>', '').replace('</b>', '').replace('&quot;', '"')
        news_text += f"{idx+1}. {clean_title}\n"

    # í”„ë¡¬í”„íŠ¸ ìƒì„¸í™” (AIê°€ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ)
    prompt = f"""
    Role: Chief Editor of 'K-ENTER 24'.
    Analyze these news titles and select exactly 12 most interesting ones.
    Output MUST be a valid JSON object with "global_insight" and an "articles" array.
    
    Raw Titles:
    {news_text}
    
    JSON Schema:
    {{
        "global_insight": "summary",
        "articles": [
            {{
                "category": "K-POP",
                "artist": "Subject",
                "title": "Headline",
                "summary": "Short summary",
                "score": 9,
                "reactions": {{"excitement": 80, "sadness": 0, "shock": 20}},
                "original_title_index": 1
            }}
        ]
    }}
    """
    try:
        res = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=AI_MODEL,
            response_format={"type": "json_object"}
        )
        return json.loads(res.choices[0].message.content)
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def run():
    print(f"=== {datetime.now()} ì‹¤ì „ ëª¨ë“œ ì‹œì‘ ===")
    all_news = []
    for keyword in SEARCH_KEYWORDS:
        items = get_naver_api_news(keyword)
        print(f"ğŸ“¡ {keyword}: {len(items)}ê±´ ë°œê²¬")
        all_news.extend(items)
    
    print(f"ğŸ” ì´ {len(all_news)}ê±´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨. AI ë¶„ì„ ì‹œì‘...")
    
    result = ai_chief_editor(all_news)
    if not result or 'articles' not in result:
        print("âŒ AIê°€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“ AIê°€ {len(result['articles'])}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘...")

    saved_count = 0
    for article in result.get('articles', []):
        idx = article.get('original_title_index', 1) - 1
        if idx < 0 or idx >= len(all_news): idx = 0
        original = all_news[idx]

        real_img = get_real_news_image(original['link'])
        
        if not real_img:
            # ì—¬ì „íˆ ì‹¤íŒ¨ ì‹œ ë³´ì¡° ìˆ˜ë‹¨ (ë„¤ì´ë²„ ë¡œê³ ë¼ë„ ì•ˆ ë‚˜ì˜¤ê²Œ í•˜ê¸° ìœ„í•´ ê°€ìˆ˜ëª…ìœ¼ë¡œ ìƒì„±)
            real_img = f"https://placehold.co/600x400/111/cyan?text={article.get('artist', 'K-News').replace(' ', '+')}"
        else:
            print(f"ğŸ“¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì„±ê³µ: {article['title'][:20]}...")

        try:
            # ì¤‘ë³µ ì²´í¬ (DBê°€ ë¹„ì–´ìˆë‹¤ë©´ ë¬´ì¡°ê±´ í†µê³¼í•´ì•¼ í•¨)
            data = {
                "category": article.get('category', 'General'),
                "artist": article.get('artist', 'Trend'),
                "title": article['title'],
                "summary": article['summary'],
                "score": article.get('score', 5),
                "link": original['link'],
                "source": "Naver News",
                "image_url": real_img,
                "reactions": article['reactions'],
                "is_published": True,
                "created_at": datetime.now().isoformat()
            }
            supabase.table("live_news").insert(data).execute()
            saved_count += 1
            print(f"âœ… ì €ì¥ë¨: {article['title'][:30]}")
        except Exception as e:
            print(f"ğŸ’¾ ì €ì¥ ì‹¤íŒ¨: {e}")

    print(f"=== ìµœì¢… ì™„ë£Œ: {saved_count}ê°œ ì—…ë°ì´íŠ¸ë¨ ===")

if __name__ == "__main__":
    run()
