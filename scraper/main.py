import os
import time
from datetime import datetime
from dotenv import load_dotenv

# ì‚¬ìš©ì ëª¨ë“ˆ ì„í¬íŠ¸
from config import CATEGORY_MAP
from scraper import crawler, ai_engine, repository, update_rankings

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def run_scraper():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ìš”ì•½ í•µì‹¬ ë¡œì§ (1íšŒ ì‹¤í–‰)"""
    print("ğŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™...")
    
    for category, keywords in CATEGORY_MAP.items():
        try:
            print(f"\nğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

            # 1. ìˆ˜ì§‘
            raw_news = []
            for kw in keywords: 
                raw_news.extend(crawler.get_naver_api_news(kw))
            
            # 2. ì¤‘ë³µ ì œê±°
            existing_links = repository.get_existing_links(category)
            
            new_candidate_news = []
            seen_links = set()
            for n in raw_news:
                if n['link'] not in existing_links and n['link'] not in seen_links:
                    new_candidate_news.append(n)
                    seen_links.add(n['link'])
            
            print(f"   ğŸ” ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ê¸°ì¡´ DB ì¤‘ë³µ ì œì™¸: {len(new_candidate_news)}ê°œ")

            if not new_candidate_news:
                continue

            # 3. AI ì„ ë³„
            selected = ai_engine.ai_category_editor(category, new_candidate_news)
            print(f"   ã„´ AI ì„ ë³„ ì™„ë£Œ: {len(selected)}ê°œ")

            # 4. ì‹ ê·œ ë‰´ìŠ¤ ë°ì´í„° ìƒì„± ë° ì €ì¥
            if selected:
                new_data_list = []
                for i, art in enumerate(selected):
                    idx = art.get('original_index')
                    if idx is None or idx >= len(new_candidate_news): continue
                    
                    orig = new_candidate_news[idx]
                    img = crawler.get_article_image(orig['link']) or f"https://placehold.co/600x400/111/cyan?text={category}"

                    new_data_list.append({
                        "rank": art.get('rank', 99), 
                        "category": category, 
                        "title": art.get('eng_title', orig['title']),
                        "summary": art.get('summary', 'Detailed summary not available.'), 
                        "link": orig['link'], 
                        "image_url": img,
                        "score": art.get('score', 5.0), 
                        "likes": 0, 
                        "dislikes": 0, 
                        "created_at": datetime.now().isoformat(),
                        "published_at": orig.get('published_at', datetime.now()).isoformat()
                    })
                
                repository.save_news(new_data_list)

            # 5. ìŠ¬ë¡¯ ê´€ë¦¬
            repository.manage_slots(category)

        except Exception as e:
            print(f"âš ï¸ Error processing category {category}: {e}")
            continue

    # í‚¤ì›Œë“œ ë¶„ì„ (ì˜µì…˜)
    try:
        print("\nğŸ“Š AI í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        titles = repository.get_recent_titles()
        if titles and hasattr(ai_engine, 'ai_analyze_keywords'):
            keywords = ai_engine.ai_analyze_keywords(titles)
            if keywords:
                repository.update_keywords_db(keywords)
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    print("ğŸ‰ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ.")

def main():
    print("ğŸš€ K-Enter AI News Bot Started...")
    
    # [1] ìˆœìœ„ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì•ˆì „ì¥ì¹˜ ì ìš©ë¨)
    update_rankings.update_rankings() 
    
    # [2] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
    run_scraper()
    
    print("âœ… All Tasks Completed.")

if __name__ == "__main__":
    main()
