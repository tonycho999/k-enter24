import os
import sys
import json
import time
import requests
from supabase import create_client, Client
from datetime import datetime, timedelta
from dotenv import load_dotenv
from groq import Groq

load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase: Client = create_client(os.environ.get("NEXT_PUBLIC_SUPABASE_URL"), os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY"))
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

MODELS_TO_TRY = ["llama-3.3-70b-versatile", "llama-3.1-70b-versatile"]

CATEGORY_MAP = {
    "k-pop": ["ì»´ë°±", "ë¹Œë³´ë“œ", "ì•„ì´ëŒ", "ë®¤ì§", "ë¹„ë””ì˜¤", "ì±Œë¦°ì§€", "í¬í† ì¹´ë“œ", "ì›”ë“œíˆ¬ì–´", "ê°€ìˆ˜"],
    "k-drama": ["ë“œë¼ë§ˆ", "ì‹œì²­ë¥ ", "ë„·í”Œë¦­ìŠ¤", "OTT", "ë°°ìš°", "ìºìŠ¤íŒ…", "ëŒ€ë³¸ë¦¬ë”©", "ì¢…ì˜"],
    "k-movie": ["ì˜í™”", "ê°œë´‰", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "ì‹œì‚¬íšŒ", "ì˜í™”ì œ", "ê´€ê°", "ë¬´ëŒ€ì¸ì‚¬"],
    "k-entertain": ["ì˜ˆëŠ¥", "ìœ íŠœë¸Œ", "ê°œê·¸ë§¨", "ì½”ë¯¸ë””ì–¸", "ë°©ì†¡", "ê°œê·¸ìš°ë¨¼"],
    "k-culture": ["í‘¸ë“œ", "ë·°í‹°", "ì›¹íˆ°", "íŒì—…ìŠ¤í† ì–´", "íŒ¨ì…˜", "ìŒì‹", "í•´ì™¸ë°˜ì‘"]
}

def get_naver_api_news(keyword):
    import urllib.parse, urllib.request
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

def get_article_image(link):
    from bs4 import BeautifulSoup
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get(link, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        og_image = soup.find('meta', property='og:image')
        return og_image['content'] if og_image else None
    except: return None

def ai_category_editor(category, news_batch):
    if not news_batch: return []
    limited_batch = news_batch[:150]
    raw_text = "\n".join([f"[{i}] {n['title']}" for i, n in enumerate(limited_batch)])
    
    prompt = f"""
    Task: Select buzzworthy news for '{category}'. 
    - Select EXACTLY 30 items if possible.
    - Rank 1-30. 
    - Translate title to English & 3-line English summary. 
    - AI Score (0.0-10.0).
    Output JSON: {{ "articles": [ {{ "original_index": 0, "rank": 1, "category": "{category}", "eng_title": "...", "summary": "...", "score": 9.5 }} ] }}
    """
    
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(
                messages=[{"role": "system", "content": "You are a professional K-Enter Editor."},
                          {"role": "user", "content": prompt}], 
                model=model, response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content).get('articles', [])
        except: continue
    return []

def run():
    print("ğŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™ (ì•ˆì •ì  30ê°œ ìœ ì§€ ëª¨ë“œ)...")
    
    for category, keywords in CATEGORY_MAP.items():
        print(f"ğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

        # 1. ìˆ˜ì§‘ (Maximum Fetch)
        raw_news = []
        for kw in keywords: raw_news.extend(get_naver_api_news(kw))
        
        # 2. DBì™€ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (Dedupe vs DB)
        # í˜„ì¬ DBì— ìˆëŠ” ëª¨ë“  ë§í¬ë¥¼ ê°€ì ¸ì™€ì„œ ë¹„êµ
        db_res = supabase.table("live_news").select("link").eq("category", category).execute()
        db_links = {item['link'] for item in db_res.data}
        
        # DBì— ì—†ëŠ” ìƒˆë¡œìš´ ê¸°ì‚¬ë§Œ í•„í„°ë§
        new_candidate_news = [n for n in raw_news if n['link'] not in db_links]
        # ë¦¬ìŠ¤íŠ¸ ë‚´ ìì²´ ì¤‘ë³µë„ ì œê±°
        new_candidate_news = list({n['link']: n for n in new_candidate_news}.values())
        
        print(f"   ğŸ” ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ì‹ ê·œ ê¸°ì‚¬: {len(new_candidate_news)}ê°œ")

        # 3. ë¶„ë¥˜ ë° í‰ì  (AI Scoring)
        selected = ai_category_editor(category, new_candidate_news)
        num_new = len(selected)
        print(f"   ã„´ AI ì„ ë³„ ì™„ë£Œ: {num_new}ê°œ")

        if num_new > 0:
            # 7. ìƒˆë¡œìš´ ê¸°ì‚¬ ë¨¼ì € ì €ì¥ (Final Upsert)
            new_data_list = []
            for art in selected:
                idx = art['original_index']
                if idx >= len(new_candidate_news): continue
                orig = new_candidate_news[idx]
                img = get_article_image(orig['link']) or f"https://placehold.co/600x400/111/cyan?text={category}"

                new_data_list.append({
                    "rank": art['rank'], "category": category, "title": art['eng_title'],
                    "summary": art['summary'], "link": orig['link'], "image_url": img,
                    "score": art['score'], "likes": 0, "dislikes": 0, "created_at": datetime.now().isoformat()
                })
            
            if new_data_list:
                supabase.table("live_news").insert(new_data_list).execute()
                print(f"   âœ… ì‹ ê·œ {len(new_data_list)}ê°œ ì‚½ì… ì™„ë£Œ.")

        # 4~6. ìŠ¬ë¡¯ ì²´í¬ ë° ì¡°ê±´ë¶€ ì‚­ì œ
        # ì‚½ì… í›„ ì „ì²´ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ 30ê°œë¡œ ë§ì¶¤
        res = supabase.table("live_news").select("id", "created_at", "score").eq("category", category).execute()
        current_articles = res.data
        
        if len(current_articles) > 30:
            now = datetime.now()
            threshold = now - timedelta(hours=24)
            
            # ê¸°ì‚¬ ë¶„ë¦¬: 24ì‹œê°„ ì§€ë‚œ ê²ƒ / ìµœì‹  ê²ƒ
            old_articles = []
            fresh_articles = []
            for a in current_articles:
                created_at = datetime.fromisoformat(a['created_at'].replace('Z', '+00:00')).replace(tzinfo=None)
                if created_at < threshold: old_articles.append(a)
                else: fresh_articles.append(a)
            
            # 5. 24ì‹œê°„ ë„˜ì€ ê¸°ì‚¬ ì‚­ì œ (30ê°œ ë  ë•Œê¹Œì§€ë§Œ)
            # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì •ë ¬
            old_articles.sort(key=lambda x: x['created_at'])
            
            delete_ids = []
            current_count = len(current_articles)
            
            for oa in old_articles:
                if current_count <= 30: break
                delete_ids.append(oa['id'])
                current_count -= 1
            
            # 6. ê·¸ë˜ë„ 30ê°œ ë„˜ìœ¼ë©´ ì ìˆ˜ ë‚®ì€ ìˆœìœ¼ë¡œ ì‚­ì œ
            if current_count > 30:
                # ë‚¨ì€ ê¸°ì‚¬ë“¤ ì¤‘ ì ìˆ˜ ë‚®ì€ ìˆœ ì •ë ¬
                remaining = [a for a in current_articles if a['id'] not in delete_ids]
                remaining.sort(key=lambda x: x['score'])
                
                for ra in remaining:
                    if current_count <= 30: break
                    delete_ids.append(ra['id'])
                    current_count -= 1

            if delete_ids:
                supabase.table("live_news").delete().in_("id", delete_ids).execute()
                print(f"   ğŸ§¹ ìŠ¬ë¡¯ ì¡°ì •: {len(delete_ids)}ê°œ ì‚­ì œ ì™„ë£Œ (ìµœì¢… 30ê°œ ìœ ì§€)")

    print(f"ğŸ‰ ì‘ì—… ì™„ë£Œ. ê° ì¹´í…Œê³ ë¦¬ 30ê°œ ìŠ¬ë¡¯ ìµœì í™”.")

if __name__ == "__main__":
    run()
