import os
import json
import time
import google.generativeai as genai
from supabase import create_client, Client
from dotenv import load_dotenv
from duckduckgo_search import DDGS

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. Supabase ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. Gemini ì„¤ì •
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)

# [í•µì‹¬] ê²€ìƒ‰ ë„êµ¬ ì—†ì´ 'ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìƒì„±' ëª¨ë¸ ì‚¬ìš© (ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨)
model = genai.GenerativeModel('gemini-1.5-flash')

CATEGORIES = {
    "K-Pop": "k-pop news latest trends ranking",
    "K-Drama": "k-drama news ratings ranking actor controversy",
    "K-Movie": "korean movie box office news actor interview",
    "K-Variety": "korean variety show ratings news funny moments",
    "K-Culture": "korea travel hot place seoul festival food trend"
}

def search_web(keyword):
    """DuckDuckGoë¥¼ ì´ìš©í•´ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"ğŸ” [Search] '{keyword}' ê²€ìƒ‰ ì¤‘...")
    results = []
    try:
        # ddg ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        with DDGS() as ddgs:
            # ë‰´ìŠ¤ ê²€ìƒ‰ (ìµœì‹ ìˆœ)
            ddg_results = list(ddgs.news(keywords=keyword, region="kr-kr", safesearch="off", max_results=15))
            
            for r in ddg_results:
                results.append(f"ì œëª©: {r.get('title')}\në§í¬: {r.get('url')}\në‚´ìš©: {r.get('body')}\nì¶œì²˜: {r.get('source')}")
                
    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return "\n\n".join(results)

def fetch_data_from_gemini(category_name, raw_data):
    """ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ Geminiì—ê²Œ ë˜ì ¸ì„œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ê²Œ ì‹œí‚µë‹ˆë‹¤."""
    print(f"ğŸ¤– [Gemini] '{category_name}' ë°ì´í„° ì •ë¦¬ ì¤‘...")
    
    prompt = f"""
    [Role]
    You are a veteran K-Entertainment journalist.
    
    [Context]
    Here is the latest raw search data about '{category_name}':
    {raw_data}

    [Task]
    Analyze the raw data above and extract the most important trends.
    Return the result in strict JSON format.

    [Requirements]
    1. **news_updates**: Select 10 most important news.
       - 'summary' must be in Korean (Hangul).
       - 'title' must be in Korean.
    2. **rankings**: Extract or infer Top 10 rankings based on the buzz.
       - If exact ranking data is missing, rank them by mention frequency.
       - Items must be unique.

    [Output Format (JSON Only)]
    {{
      "news_updates": [
        {{
          "keyword": "Main Subject (e.g. NewJeans)",
          "title": "News Title (Korean)",
          "summary": "Summary (Korean, 150 chars)",
          "link": "Source URL from raw data"
        }},
        ...
      ],
      "rankings": [
        {{ "rank": 1, "title": "Song/Drama/Movie Title", "meta": "Artist/Actor/Channel" }},
        ...
      ]
    }}
    """
    
    try:
        response = model.generate_content(prompt)
        text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    except Exception as e:
        print(f"âŒ [Error] Gemini ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def update_database(category, data):
    # 1. ë‰´ìŠ¤ ì €ì¥
    news_list = data.get("news_updates", [])
    if news_list:
        clean_news = []
        for item in news_list:
            clean_news.append({
                "category": category,
                "keyword": item.get("keyword", category),
                "title": item.get("title", ""),
                "summary": item.get("summary", ""),
                "link": item.get("link", ""),
                "created_at": "now()"
            })
        
        try:
            supabase.table("search_archive").upsert(clean_news, on_conflict="category,keyword,title").execute()
            supabase.table("live_news").upsert(clean_news, on_conflict="category,keyword,title").execute()
            print(f"   ğŸ’¾ ë‰´ìŠ¤ {len(clean_news)}ê°œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    # 2. ë¡¤ë§ ì—…ë°ì´íŠ¸ (30ê°œ ìœ ì§€)
    try:
        res = supabase.table("live_news").select("id").eq("category", category).order("created_at", desc=True).execute()
        all_ids = [row['id'] for row in res.data]
        if len(all_ids) > 30:
            ids_to_delete = all_ids[30:]
            supabase.table("live_news").delete().in_("id", ids_to_delete).execute()
    except Exception:
        pass

    # 3. ë­í‚¹ ì €ì¥
    rank_list = data.get("rankings", [])
    if rank_list:
        clean_ranks = []
        for item in rank_list:
            clean_ranks.append({
                "category": category,
                "rank": item.get("rank"),
                "title": item.get("title"),
                "meta_info": item.get("meta", ""),
                "updated_at": "now()"
            })
        try:
            supabase.table("live_rankings").upsert(clean_ranks, on_conflict="category,rank").execute()
            print(f"   ğŸ† ë­í‚¹ ê°±ì‹  ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ë­í‚¹ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    print("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° AI ìš”ì•½ ì‹œì‘ (DuckDuckGo + Gemini)")
    
    for category, search_keyword in CATEGORIES.items():
        # 1. DuckDuckGoë¡œ ê²€ìƒ‰
        raw_text = search_web(search_keyword)
        
        if len(raw_text) < 50:
            print(f"âš ï¸ {category} ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±. ê±´ë„ˆëœ€.")
            continue

        # 2. Geminiì—ê²Œ ìš”ì•½ ìš”ì²­
        data = fetch_data_from_gemini(category, raw_text)
        
        # 3. DB ì €ì¥
        if data:
            update_database(category, data)
        
        time.sleep(3) # ë°´ ë°©ì§€ìš© ëŒ€ê¸°

    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()
