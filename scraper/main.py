import sys
import os
import time
from datetime import datetime, timedelta
from dateutil import parser
from dotenv import load_dotenv

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS

load_dotenv()

TARGET_RANK_LIMIT = 30 

def is_within_24h(date_str):
    if not date_str: return False
    try:
        pub_date = parser.parse(date_str)
        if pub_date.tzinfo:
            pub_date = pub_date.replace(tzinfo=None)
        now = datetime.now()
        diff = now - pub_date
        return diff <= timedelta(hours=24)
    except:
        return False

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # [1ë‹¨ê³„] ì”¨ì•— ë°ì´í„° ìˆ˜ì§‘
        raw_text_data = [] 
        
        try:
            for seed in seeds:
                # [ìˆ˜ì •] 20ê°œì”©ë§Œ ê°€ì ¸ì˜´ (í† í° ì ˆì•½)
                news_items = crawler.get_naver_api_news(seed, display=20)
                
                for item in news_items:
                    if is_within_24h(item.get('pubDate')):
                        combined_text = f"Title: {item['title']}\nSummary: {item['description']}"
                        raw_text_data.append(combined_text)
            
            # [ìˆ˜ì •] AIì—ê²Œ ë³´ë‚¼ ë°ì´í„°ë¥¼ ìµœëŒ€ 60ê°œë¡œ ì œí•œ (API ì œí•œ ë°©ì§€)
            raw_text_data = raw_text_data[:60]
            
            print(f"   ğŸŒ± 24ì‹œê°„ ë‚´ ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì§‘: {len(raw_text_data)}ê°œ (AI ì…ë ¥ìš©)")
            
            if len(raw_text_data) < 5:
                print("   âš ï¸ ê¸°ì‚¬ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
                
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            continue
        
        # [2ë‹¨ê³„] AI í‚¤ì›Œë“œ ì¶”ì¶œ
        top_entities = ai_engine.extract_top_entities(category, "\n".join(raw_text_data))
        
        if not top_entities: 
            print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ (AI ì‘ë‹µ ì—†ìŒ)")
            continue
            
        print(f"   ğŸ’ ì¶”ì¶œëœ í‚¤ì›Œë“œ (Top 5): {', '.join([e['keyword'] for e in top_entities[:5]])}...")

        # [3ë‹¨ê³„] í‚¤ì›Œë“œë³„ ì‹¬ì¸µ ë¶„ì„
        category_news_list = []
        target_list = top_entities[:TARGET_RANK_LIMIT]
        
        for rank, entity in enumerate(target_list):
            kw = entity.get('keyword')
            k_type = entity.get('type', 'content')
            
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ({k_type}) ë¶„ì„ ì¤‘...")
            
            try:
                # [ìˆ˜ì •] 10ê°œë§Œ ê²€ìƒ‰
                raw_articles = crawler.get_naver_api_news(kw, display=10)
                if not raw_articles: continue

                full_contents = []
                main_image = None
                valid_article_count = 0
                
                for art in raw_articles:
                    if not is_within_24h(art.get('pubDate')):
                        continue
                        
                    text, img = crawler.get_article_data(art['link'], target_keyword=kw)
                    
                    if text: 
                        full_contents.append(text)
                        valid_article_count += 1
                        if not main_image and img:
                            if img.startswith("http://"): img = img.replace("http://", "https://")
                            main_image = img
                            
                    # [ìˆ˜ì •] 3ê°œë§Œ ëª¨ìœ¼ë©´ ì¶©ë¶„ (ì†ë„ í–¥ìƒ)
                    if valid_article_count >= 3:
                        break

                if not full_contents:
                    print(f"      â˜ï¸ '{kw}': ìœ íš¨ ê¸°ì‚¬ ì—†ìŒ (Skip)")
                    continue

                # [4ë‹¨ê³„] AI ë¸Œë¦¬í•‘
                briefing = ai_engine.synthesize_briefing(kw, full_contents)
                
                if not briefing:
                    print(f"      ğŸ—‘ï¸ '{kw}': ë‚´ìš© ë¶€ì‹¤ë¡œ íê¸°")
                    continue
                
                ai_score = round(9.9 - (rank * 0.1), 1)
                if ai_score < 7.0: ai_score = 7.0

                final_img = main_image or f"https://placehold.co/600x400/111/cyan?text={kw}"

                news_item = {
                    "category": category,
                    "rank": rank + 1,
                    "keyword": kw,
                    "type": k_type,
                    "title": f"[{kw}] News Update",
                    "summary": briefing,
                    "link": None,
                    "image_url": final_img,
                    "score": ai_score,
                    "likes": 0, "dislikes": 0,
                    "created_at": datetime.now().isoformat(),
                    "published_at": datetime.now().isoformat()
                }
                category_news_list.append(news_item)
                # [ìˆ˜ì •] API ë¶€í•˜ ì¤„ì´ê¸° ìœ„í•´ 2ì´ˆ ëŒ€ê¸°
                time.sleep(2.0) 
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue

        # [5ë‹¨ê³„] DB ì €ì¥
        if category_news_list:
            print(f"   ğŸ’¾ ì €ì¥ ì‹œì‘: ì´ {len(category_news_list)}ê°œ")
            repository.refresh_live_news(category, category_news_list)
            
            content_only_list = [n for n in category_news_list if n.get('type') == 'content']
            # ì»¨í…ì¸  íƒ€ì…ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì„ì—¬ë„ ë‚˜ì˜¤ê²Œ í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ì¡°ì¹˜ (5ê°œ ì´í•˜ì¼ ê²½ìš°)
            if len(content_only_list) < 5:
                repository.update_sidebar_rankings(category, category_news_list[:10])
            else:
                repository.update_sidebar_rankings(category, content_only_list[:10])
            
            high_score_news = [n for n in category_news_list if n['score'] >= 7.0]
            if high_score_news:
                repository.save_to_archive(high_score_news)

    print("\nğŸ‰ ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

if __name__ == "__main__":
    run_master_scraper()
