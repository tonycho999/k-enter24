import json
import re
import os
import time
from datetime import datetime, timedelta
from news_api import NewsEngine
from naver_api import NaverManager
from database import DatabaseManager
from supabase import create_client

# ---------------------------------------------------------
# [ì„¤ì •] ì‹¤í–‰ ì‚¬ì´í´
# ---------------------------------------------------------
# K-Popì€ ë§¤ì‹œê°„ ì°¨íŠ¸ ê°±ì‹ , ë‚˜ë¨¸ì§€ëŠ” íŠ¹ì • ì‹œê°„ì—ë§Œ ì°¨íŠ¸ ê°±ì‹ 
# í•˜ì§€ë§Œ "ì¸ë¬¼ ë‰´ìŠ¤"ëŠ” ë§¤ì‹œê°„ íŠ¸ë Œë“œë¥¼ ì²´í¬í•©ë‹ˆë‹¤.
TARGET_COUNTS_FOR_OTHERS = [5, 17] 

def clean_json_text(text):
    match = re.search(r"```(?:json)?\s*(.*)\s*```", text, re.DOTALL)
    if match: text = match.group(1)
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1: return text[start:end+1]
    return text.strip()

# ---------------------------------------------------------
# [DB ì—°ë™]
# ---------------------------------------------------------
supa_url = os.environ.get("SUPABASE_URL")
supa_key = os.environ.get("SUPABASE_KEY")
supabase = create_client(supa_url, supa_key) if supa_url and supa_key else None

def get_run_count():
    if not supabase: return 0
    try:
        res = supabase.table('system_status').select('run_count').eq('id', 1).single().execute()
        return res.data['run_count'] if res.data else 0
    except: return 0

def update_run_count(current):
    if not supabase: return
    next_count = current + 1
    if next_count >= 24: next_count = 0
    try:
        supabase.table('system_status').upsert({'id': 1, 'run_count': next_count}).execute()
        print(f"ğŸ”„ Cycle Count Updated: {current} -> {next_count}")
    except Exception as e:
        print(f"âš ï¸ Failed to update run count: {e}")

# ---------------------------------------------------------
# [Helper] ì´ì „ì— ì‘ì„±ëœ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ/ì‹ ê·œ ì²´í¬ìš©)
# ---------------------------------------------------------
def get_recent_keywords(category):
    """
    ìµœê·¼ 12ì‹œê°„ ë‚´ì— í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ ì‘ì„±ëœ ì¸ë¬¼ ì´ë¦„(keyword)ì„ ê°€ì ¸ì˜´
    """
    if not supabase: return []
    try:
        # 12ì‹œê°„ ì „ ì‹œê°„ êµ¬í•˜ê¸°
        time_limit = (datetime.utcnow() - timedelta(hours=12)).isoformat()
        
        res = supabase.table('search_archive') \
            .select('keyword') \
            .eq('category', category) \
            .gte('created_at', time_limit) \
            .execute()
            
        if res.data:
            return set([item['keyword'] for item in res.data])
        return set()
    except Exception as e:
        print(f"âš ï¸ Failed to fetch history: {e}")
        return set()

# ---------------------------------------------------------
# [ë©”ì¸ ë¡œì§]
# ---------------------------------------------------------
def run_automation():
    run_count = get_run_count()
    print(f"ğŸš€ Automation Started (Cycle: {run_count}/23)")
    
    db = DatabaseManager()
    engine = NewsEngine(run_count) # Run count ì „ë‹¬ (í‚¤ ë¡œí…Œì´ì…˜ìš©)
    naver = NaverManager()
    
    # Key 1ë²ˆ ì‚¬ìš© ì—¬ë¶€ (ì°¨íŠ¸ ê°±ì‹ ìš©)
    is_key1 = engine.is_using_primary_key()
    
    categories = ["k-pop", "k-drama", "k-movie", "k-entertain", "k-culture"]

    for cat in categories:
        print(f"\n[{cat}] Analyzing Trends...")
        
        # 1. ìµœê·¼ì— ë‹¤ë£¬ ì¸ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì‹ ê·œ ì§„ì… íŒë³„ìš©)
        recent_people = get_recent_keywords(cat)

        try:
            # -----------------------------------------------------------
            # Step 1. ë¦¬ìŠ¤íŠ¸ í™•ë³´ (Top 10 Chart + Top 30 People List)
            # -----------------------------------------------------------
            list_json = engine.get_rankings_list(cat)
            
            cleaned_list = clean_json_text(list_json)
            if not cleaned_list or cleaned_list == "{}":
                print(f"âš ï¸ [{cat}] No list data returned. Skipping.")
                continue
                
            parsed_list = json.loads(cleaned_list)
            
            # -----------------------------------------------------------
            # Step 2. Top 10 ì°¨íŠ¸ ì €ì¥
            # -----------------------------------------------------------
            # ê·œì¹™: K-POPì€ ë§¤ì‹œê°„, ë‚˜ë¨¸ì§€ëŠ” Key 1ë²ˆì¼ ë•Œë§Œ ì €ì¥
            should_update_chart = (cat == 'k-pop') or is_key1
            
            top10_data = parsed_list.get('top10', [])
            if top10_data and should_update_chart:
                print(f"  > ğŸ“Š Saving Top 10 Chart ({len(top10_data)} items)...")
                db_data = []
                for item in top10_data:
                    db_data.append({
                        "category": cat,
                        "rank": item.get('rank'),
                        "title": item.get('title'),
                        "meta_info": item.get('info', ''),
                        "score": 0
                    })
                db.save_rankings(db_data)
            elif top10_data:
                print(f"  > â© Skipping Chart Update (Not Key 1).")

            # -----------------------------------------------------------
            # Step 3. ì¸ë¬¼ë³„ ê¸°ì‚¬ ì‘ì„± (ì¡°ê±´ë¶€)
            # -----------------------------------------------------------
            people_list = parsed_list.get('people', [])
            if people_list:
                print(f"  > ğŸ‘¥ Reviewing {len(people_list)} People for updates...")
                
                live_news_buffer = [] 

                for person in people_list:
                    rank = person.get('rank')
                    name_en = person.get('name_en')
                    name_kr = person.get('name_kr')
                    
                    if not name_en or not rank: continue
                    if not name_kr: name_kr = name_en
                    
                    # [ì¡°ê±´ ë¡œì§]
                    # 1ìœ„~3ìœ„: ë¬´ì¡°ê±´ ì‘ì„± (ë³€í™” ì—†ì–´ë„ ìµœì‹  ì´ìŠˆ ì²´í¬)
                    # 4ìœ„~30ìœ„: ìµœê·¼(12ì‹œê°„)ì— ë‹¤ë£¬ ì  ì—†ëŠ” "ì‹ ê·œ ì§„ì…ì"ë§Œ ì‘ì„±
                    
                    should_write = False
                    reason = ""
                    
                    if rank <= 3:
                        should_write = True
                        reason = "Top 3 Rank"
                    elif name_en not in recent_people:
                        should_write = True
                        reason = "New Entry"
                    
                    if should_write:
                        print(f"    -> ğŸ“ Processing Rank #{rank}: {name_en} ({reason})...")
                        
                        # (1) ì‹¬ì¸µ ì·¨ì¬ (Perplexity) - ê¸°ì‚¬ ê°œìˆ˜ ìë™ ì¡°ì ˆë¨
                        # fetch_article_details ë‚´ë¶€ì—ì„œ rankì— ë”°ë¼ 4ê°œ/3ê°œ/2ê°œ ì½ìŒ
                        facts = engine.fetch_article_details(name_kr, name_en, cat, rank)
                        
                        if "Failed" in facts:
                            print(f"       âš ï¸ Skip: Facts collection failed.")
                            continue

                        # (2) ê¸°ì‚¬ ì‘ì„± (Groq)
                        full_text = engine.edit_with_groq(name_en, facts, cat)
                        
                        # (3) ë°ì´í„° íŒŒì‹±
                        score = 70
                        if "###SCORE:" in full_text:
                            try:
                                parts = full_text.split("###SCORE:")
                                full_text = parts[0].strip()
                                import re
                                m = re.search(r'\d+', parts[1])
                                if m: score = int(m.group())
                            except: pass
                            
                        lines = full_text.split('\n')
                        title = lines[0].replace('Headline:', '').strip()
                        summary = "\n".join(lines[1:]).strip()
                        img_url = naver.get_image(name_kr)
                        
                        article_data = {
                            "category": cat,
                            "keyword": name_en,
                            "title": title,
                            "summary": summary,
                            "link": "", # ë§í¬ ì—†ìŒ
                            "image_url": img_url,
                            "score": score,
                            "likes": 0,
                            "query": f"{cat} top 30 rank {rank}",
                            "raw_result": str(person),
                            "run_count": run_count
                        }
                        
                        # (4) DB ì €ì¥
                        db.save_to_archive(article_data)
                        
                        live_news_buffer.append({
                            "category": article_data['category'],
                            "keyword": article_data['keyword'],
                            "title": article_data['title'],
                            "summary": article_data['summary'],
                            "link": "",
                            "image_url": article_data['image_url'],
                            "score": score,
                            "likes": 0
                        })
                        
                        # API ì†ë„ ì¡°ì ˆì„ ìœ„í•´ ì•½ê°„ ëŒ€ê¸° (ë„ˆë¬´ ë¹ ë¥´ë©´ ì—ëŸ¬ë‚¨)
                        time.sleep(1) 
                    else:
                        pass # ì´ë¯¸ ë‹¤ë¤˜ê³  ìˆœìœ„ë„ 4ìœ„ ë°–ì´ë©´ ìŠ¤í‚µ

                # Live News í…Œì´ë¸” ì—…ë°ì´íŠ¸ (ìƒˆë¡œ ì“´ ê¸°ì‚¬ë“¤)
                if live_news_buffer:
                    db.save_live_news(live_news_buffer)
                    print(f"  > âœ… Published {len(live_news_buffer)} New Articles.")
                else:
                    print("  > ğŸ’¤ No new articles needed.")

        except Exception as e:
            print(f"âŒ [{cat}] Critical Error: {e}")

    update_run_count(run_count)

if __name__ == "__main__":
    run_automation()
