import json
import re
import os
import time
from news_api import NewsEngine
from naver_api import NaverManager
from database import DatabaseManager
from supabase import create_client

# ì„¤ì •
TARGET_COUNTS_FOR_OTHERS = [5, 17] 

def clean_json_text(text):
    match = re.search(r"```(?:json)?\s*(.*)\s*```", text, re.DOTALL)
    if match: text = match.group(1)
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1: return text[start:end+1]
    return text.strip()

# DB ì—°ê²°
supa_url = os.environ.get("SUPABASE_URL")
supa_key = os.environ.get("SUPABASE_KEY")
supabase = create_client(supa_url, supa_key) if supa_url and supa_key else None

def get_run_count():
    if not supabase: return 0
    try:
        res = supabase.table('system_status').select('run_count').eq('id', 1).single().execute()
        return res.data['run_count'] if res.data else 0
    except: return 0

def update_run_count(current):
    if not supabase: return
    next_count = current + 1
    if next_count >= 24: next_count = 0
    try:
        supabase.table('system_status').upsert({'id': 1, 'run_count': next_count}).execute()
    except Exception as e: print(f"âš ï¸ Update Count Error: {e}")

# ---------------------------------------------------------
# [Helper] ì´ì „ ì‹¤í–‰ì˜ ìˆœìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
# ---------------------------------------------------------
def get_previous_rank_map(db_manager, category):
    """
    DB(search_archive)ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ê°€ì¥ ìµœê·¼ 30ê°œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™€ì„œ
    { "keyword": rank } í˜•íƒœì˜ ë§µì„ ë§Œë“­ë‹ˆë‹¤.
    """
    try:
        # DB ë§¤ë‹ˆì €ì— get_latest_articles ë©”ì„œë“œê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜ ì§ì ‘ ì¿¼ë¦¬
        # ì—¬ê¸°ì„œëŠ” supabase clientë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„
        res = supabase.table('search_archive') \
            .select('keyword, raw_result') \
            .eq('category', category) \
            .order('created_at', desc=True) \
            .limit(50) \
            .execute()
            
        rank_map = {}
        if res.data:
            for item in res.data:
                # raw_resultì— ì €ì¥ëœ JSON ë“±ì—ì„œ rankë¥¼ íŒŒì‹±í•˜ê±°ë‚˜
                # ë‹¨ìˆœíˆ "ì´ì „ì— ì¡´ì¬í–ˆëŠ”ì§€" ì—¬ë¶€ë§Œ ì²´í¬í•´ë„ ë©ë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” keyword(ì´ë¦„)ê°€ ì¡´ì¬í•˜ë©´ ì´ì „ ìˆœìœ„ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤ê³  ê°€ì •.
                # í¸ì˜ìƒ 'ì¡´ì¬ ì—¬ë¶€'ë§Œ ì²´í¬í•˜ê±°ë‚˜, ì •í™•í•œ Rank ë¹„êµë¥¼ ìœ„í•´ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥.
                rank_map[item['keyword']] = 0 # 0ì€ 'ì¡´ì¬í–ˆìŒ' í‘œì‹œ
        return rank_map
    except:
        return {}

def run_automation():
    run_count = get_run_count()
    print(f"ğŸš€ Automation Started (Cycle: {run_count}/23)")
    
    db = DatabaseManager()
    engine = NewsEngine(run_count)
    naver = NaverManager()
    
    is_key1 = engine.is_using_primary_key()
    
    categories = ["k-pop", "k-drama", "k-movie", "k-entertain", "k-culture"]

    for cat in categories:
        # [ìŠ¤ì¼€ì¤„ ì²´í¬] ì¸ë¬¼ ë‰´ìŠ¤ëŠ” ë§¤ì‹œê°„ ê°±ì‹  ì²´í¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ ì•ˆí•¨.
        # ë‹¨, Top 10 ì°¨íŠ¸ ì €ì¥ ì—¬ë¶€ëŠ” ì•„ë˜ì—ì„œ ê²°ì •.
        
        print(f"\n[{cat}] Analyzing Trends...")
        
        # 1. ì´ì „ ì‹œê°„ëŒ€ ìˆœìœ„ ì •ë³´ ë¡œë“œ (ë¹„êµìš©)
        prev_ranks = get_previous_rank_map(db, cat)

        try:
            # 2. Perplexityë¡œ 'Top 10 ì°¨íŠ¸' + 'Top 30 ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸' ê°€ì ¸ì˜¤ê¸°
            list_json = engine.get_rankings_list(cat)
            parsed_list = json.loads(clean_json_text(list_json))
            
            # -----------------------------------------------------------
            # A. Top 10 Chart ì²˜ë¦¬ (ì¡°ê±´ë¶€ ì €ì¥)
            # -----------------------------------------------------------
            # ì¡°ê±´: K-POPì€ ë§¤ì‹œê°„, ë‚˜ë¨¸ì§€ëŠ” Key 1ë²ˆì¼ ë•Œë§Œ
            should_update_chart = (cat == 'k-pop') or is_key1
            
            top10_data = parsed_list.get('top10', [])
            if top10_data and should_update_chart:
                print(f"  > ğŸ“Š Saving Top 10 Chart ({len(top10_data)} items)...")
                db_data = []
                for item in top10_data:
                    db_data.append({
                        "category": cat,
                        "rank": item.get('rank'),
                        "title": item.get('title'),
                        "meta_info": item.get('info', ''),
                        "score": 0
                    })
                db.save_rankings(db_data)
            elif top10_data:
                print(f"  > â© Skipping Chart Update (Not Key 1 & Not K-Pop).")

            # -----------------------------------------------------------
            # B. Top 30 People Articles ì²˜ë¦¬ (ì¡°ê±´ë¶€ ì‘ì„±)
            # -----------------------------------------------------------
            people_list = parsed_list.get('people', [])
            if people_list:
                print(f"  > ğŸ‘¥ Checking {len(people_list)} People for updates...")
                
                processed_count = 0
                live_news_buffer = [] # ë¼ì´ë¸Œ ë‰´ìŠ¤ìš© ë²„í¼

                for person in people_list:
                    rank = person.get('rank')
                    name_en = person.get('name_en')
                    name_kr = person.get('name_kr')
                    if not name_en or not rank: continue
                    
                    # [ê²°ì • ë¡œì§] ê¸°ì‚¬ë¥¼ ìƒˆë¡œ ì“¸ ê²ƒì¸ê°€?
                    # 1. 1~3ìœ„: ë¬´ì¡°ê±´ ì‘ì„±
                    # 2. 4~30ìœ„: ì´ì „ì— ì—†ë˜ ì‚¬ëŒì´ê±°ë‚˜(New), ìˆœìœ„ ë³€ë™ì´ ìˆì„ ë•Œ(ë¡œì§ìƒ ë‹¨ìˆœí™”í•˜ì—¬ 'ì´ë¦„ì´ ì—†ìœ¼ë©´'ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥)
                    # (ì •í™•í•œ Rank ë¹„êµë¥¼ ì›í•˜ì‹œë©´ prev_ranksì— rankê°’ ì €ì¥í•´ì„œ ë¹„êµí•˜ë©´ ë¨)
                    
                    should_write = False
                    if rank <= 3:
                        should_write = True
                    else:
                        # ì´ì „ì— ì—†ë˜ ì‚¬ëŒì´ë©´ ì‘ì„±
                        if name_en not in prev_ranks:
                            should_write = True
                        # (ì„ íƒ) ìˆœìœ„ ë³€ë™ ë¡œì§ì„ ì¶”ê°€í•˜ë ¤ë©´ ì—¬ê¸°ì„œ ë¹„êµ
                    
                    if should_write:
                        print(f"    -> ğŸ“ Writing Article: #{rank} {name_en}...")
                        
                        # 2-1. ì‹¬ì¸µ ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ (Perplexity)
                        # ì—¬ê¸°ì„œ rankë¥¼ ë„˜ê²¨ì¤˜ì„œ 4ê°œ/3ê°œ/2ê°œ ê¸°ì‚¬ë¥¼ ì½ê²Œ í•¨
                        facts = engine.fetch_article_details(name_kr, name_en, cat, rank)
                        
                        # 2-2. ê¸°ì‚¬ ì‘ì„± (Groq)
                        full_text = engine.edit_with_groq(name_en, facts, cat)
                        
                        # 2-3. íŒŒì‹± ë° ì €ì¥
                        score = 70
                        if "###SCORE:" in full_text:
                            try:
                                parts = full_text.split("###SCORE:")
                                full_text = parts[0].strip()
                                import re
                                m = re.search(r'\d+', parts[1])
                                if m: score = int(m.group())
                            except: pass
                            
                        lines = full_text.split('\n')
                        title = lines[0].replace('Headline:', '').strip()
                        summary = "\n".join(lines[1:]).strip()
                        img_url = naver.get_image(name_kr)
                        
                        article_data = {
                            "category": cat,
                            "keyword": name_en,
                            "title": title,
                            "summary": summary,
                            "link": "", # ë§í¬ ìˆ˜ì§‘ ì•ˆ í•¨
                            "image_url": img_url,
                            "score": score,
                            "likes": 0,
                            "query": f"{cat} top 30 rank {rank}",
                            "raw_result": json.dumps(person), # ì›ë³¸ ë°ì´í„° ì €ì¥
                            "run_count": run_count
                        }
                        
                        # ì•„ì¹´ì´ë¸Œ ì €ì¥ (ìƒˆë¡œ ì“´ ê¸°ì‚¬ë§Œ)
                        db.save_to_archive(article_data)
                        
                        # ë¼ì´ë¸Œ ë°ì´í„° ì¤€ë¹„
                        live_news_buffer.append({
                            "category": cat,
                            "keyword": name_en,
                            "title": title,
                            "summary": summary,
                            "link": "",
                            "image_url": img_url,
                            "score": score,
                            "likes": 0
                        })
                        processed_count += 1
                        
                # [ì¤‘ìš”] ë¼ì´ë¸Œ ë‰´ìŠ¤ í…Œì´ë¸” ì—…ë°ì´íŠ¸
                # ë§¤ì‹œê°„ ìƒˆë¡œ ì“´ ê¸°ì‚¬ + (ì“°ì§€ ì•Šì•˜ë”ë¼ë„ ìˆœìœ„ê¶Œì¸ ê¸°ì‚¬ë“¤ì€ DBì—ì„œ ë¶ˆëŸ¬ì™€ì•¼ ì™„ë²½í•˜ì§€ë§Œ)
                # ìš”ì²­í•˜ì‹ ëŒ€ë¡œ "ìƒˆë¡œ ì‘ì„±ëœ ê¸°ì‚¬" ìœ„ì£¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜, 
                # ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ë®ì–´ì”Œì›ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 'ìƒˆë¡œ ì“´ ê²ƒ'ë§Œ ë¼ì´ë¸Œì— ì˜¬ë¦½ë‹ˆë‹¤.
                if live_news_buffer:
                    db.save_live_news(live_news_buffer)
                    print(f"  > âœ… Updated {len(live_news_buffer)} Live Articles.")
                    
        except Exception as e:
            print(f"âŒ [{cat}] Error: {e}")

    update_run_count(run_count)

if __name__ == "__main__":
    run_automation()
