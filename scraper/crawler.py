import os
import json
import urllib.parse
import urllib.request
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime

def get_naver_api_news(keyword):
    """ë„¤ì´ë²„ API ë‰´ìŠ¤ ê²€ìƒ‰ (íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€)"""
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=date"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    
    try:
        # [ì¤‘ìš”] timeout=10 ì¶”ê°€: 10ì´ˆ ë™ì•ˆ ì‘ë‹µ ì—†ìœ¼ë©´ í¬ê¸°
        print(f"ğŸ“¡ ë„¤ì´ë²„ API í˜¸ì¶œ ì¤‘: {keyword}...")
        res = urllib.request.urlopen(req, timeout=10) 
        items = json.loads(res.read().decode('utf-8')).get('items', [])
        
        valid_items = []
        now = datetime.now()
        threshold = now - timedelta(hours=24)

        for item in items:
            try:
                pub_date = parsedate_to_datetime(item['pubDate']).replace(tzinfo=None)
                if pub_date < threshold:
                    continue
                item['published_at'] = pub_date
                valid_items.append(item)
            except:
                continue

        return valid_items

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ API ì—ëŸ¬ ({keyword}): {e}")
        return []

def get_article_image(link):
    """ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ë¡œê·¸ ë° íƒ€ì„ì•„ì›ƒ ê°•í™”)"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        # [í™•ì¸] requests.get í˜¸ì¶œ ì‹œ timeout=5 ì„¤ì • (5ì´ˆ)
        # ì–´ë””ì„œ ë©ˆì¶”ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ í”„ë¦°íŠ¸ ì¶”ê°€
        print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘: {link[:50]}...") 
        res = requests.get(link, headers=headers, timeout=5)
        
        if res.status_code != 200:
            return None

        soup = BeautifulSoup(res.text, 'html.parser')
        candidates = []

        # 1. ë³¸ë¬¸ ì˜ì—­ ìš°ì„  íƒìƒ‰
        main_content = soup.select_one('#dic_area, #articleBodyContents, .article_view, #articeBody, .news_view')
        if main_content:
            imgs = main_content.find_all('img')
            for i in imgs:
                src = i.get('src') or i.get('data-src')
                if src and 'http' in src:
                    width = i.get('width')
                    if width and width.isdigit() and int(width) < 200: continue
                    candidates.append(src)

        # 2. ë©”íƒ€ íƒœê·¸ íƒìƒ‰
        og = soup.find('meta', property='og:image')
        if og and og.get('content'): candidates.append(og['content'])

        # 3. ë¶ˆëŸ‰ ì´ë¯¸ì§€ í•„í„°ë§
        for img_url in candidates:
            bad_keywords = r'logo|icon|button|share|banner|thumb|profile|default|ranking|news_stand|ssl.pstatic.net'
            if re.search(bad_keywords, img_url, re.IGNORECASE): continue
            return img_url
            
        return None
    except Exception as e:
        print(f"   âš ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None
