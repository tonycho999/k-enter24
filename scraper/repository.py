from datetime import datetime, timedelta
from dateutil.parser import isoparse
from config import supabase, CATEGORY_MAP

def get_existing_links(category):
    """ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ë§í¬ ì¡°íšŒ"""
    res = supabase.table("live_news").select("link").eq("category", category).execute()
    return {item['link'] for item in res.data}

def save_news(news_list):
    """
    ë‰´ìŠ¤ ì €ì¥: 
    1. ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ë§í¬ ê¸°ì¤€)
    2. ì ìˆ˜ 4.0ì  ë¯¸ë§Œ ì œê±°
    3. ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±° (ì‹œê°ì  ë‹¤ì–‘ì„± í™•ë³´)
    """
    if not news_list: return
    
    seen_links = set()
    seen_images = set() 
    unique_list = []
    
    for item in news_list:
        # [ê·œì¹™ 1] ì ìˆ˜ 4.0 ë¯¸ë§Œì€ ì €ì¥ ì•ˆ í•¨
        if item.get('score', 0) < 4.0:
            continue

        link = item['link']
        img_url = item.get('image_url', '')

        # [ê·œì¹™ 2] ë§í¬ ì¤‘ë³µ ì²´í¬
        if link in seen_links:
            continue

        # [ê·œì¹™ 3] ì´ë¯¸ì§€ ì¤‘ë³µ ì²´í¬
        if img_url and "placehold.co" not in img_url:
            if img_url in seen_images:
                continue
            seen_images.add(img_url)

        # [ìˆ˜ì •] rank í•„ë“œ ì œê±° (í•„ìš” ì—†ìŒ)
        if 'rank' in item: del item['rank']
            
        unique_list.append(item)
        seen_links.add(link)
            
    if not unique_list:
        print("   â„¹ï¸ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ì ìˆ˜ ë¯¸ë‹¬, ì¤‘ë³µ ë§í¬, ë˜ëŠ” ì¤‘ë³µ ì´ë¯¸ì§€).")
        return

    try:
        supabase.table("live_news").upsert(unique_list, on_conflict="link").execute()
        print(f"   âœ… ì‹ ê·œ {len(unique_list)}ê°œ DB ì €ì¥ ì™„ë£Œ (ì´ë¯¸ì§€ ì¤‘ë³µ ì œê±°ë¨).")
    except Exception as e:
        print(f"   âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def manage_slots(category):
    """
    [ìŠ¬ë¡¯ ê´€ë¦¬] 30ê°œ ìœ ì§€ ë¡œì§
    - 'ì‘ì„±ì¼(published_at)' ê¸°ì¤€ 24ì‹œê°„ ì§€ë‚œ ê¸°ì‚¬ ìš°ì„  ì‚­ì œ
    - ê·¸ë˜ë„ ë„˜ìœ¼ë©´ ì ìˆ˜ ë‚®ì€ ìˆœ ì‚­ì œ
    - [ìˆ˜ì •] ë­í‚¹(Rank) ì—…ë°ì´íŠ¸ ë¡œì§ ì œê±°
    """
    res = supabase.table("live_news").select("*").eq("category", category).execute()
    all_articles = res.data
    total_count = len(all_articles)
    
    print(f"   ğŸ“Š {category.upper()}: í˜„ì¬ {total_count}ê°œ (ëª©í‘œ: 30ê°œ)")

    # 30ê°œ ì´í•˜ë¼ë©´ ì‚­ì œí•  ê²ƒë„, ë­í‚¹ ë§¤ê¸¸ ê²ƒë„ ì—†ìœ¼ë¯€ë¡œ ì¢…ë£Œ
    if total_count <= 30:
        return

    # --- ì‚­ì œ ë¡œì§ ---
    delete_ids = []
    now = datetime.now()
    threshold = now - timedelta(hours=24) 
    
    def get_news_time(item):
        ts = item.get('published_at') or item.get('created_at')
        try: return isoparse(ts).replace(tzinfo=None)
        except: return datetime(2000, 1, 1)

    all_articles.sort(key=get_news_time)

    remaining_count = total_count
    
    # 1. 'ì‘ì„±ì¼' ê¸°ì¤€ 24ì‹œê°„ ì§€ë‚œ ê¸°ì‚¬ ìš°ì„  ì‚­ì œ
    for art in all_articles:
        if remaining_count <= 30: break
        
        art_date = get_news_time(art)

        if art_date < threshold:
            delete_ids.append(art['id'])
            remaining_count -= 1

    # 2. ê·¸ë˜ë„ ë§ìœ¼ë©´ ì ìˆ˜ ë‚®ì€ ìˆœ ì‚­ì œ
    if remaining_count > 30:
        survivors = [a for a in all_articles if a['id'] not in delete_ids]
        survivors.sort(key=lambda x: x.get('score', 0)) 
        
        for art in survivors:
            if remaining_count <= 30: break
            delete_ids.append(art['id'])
            remaining_count -= 1

    if delete_ids:
        supabase.table("live_news").delete().in_("id", delete_ids).execute()
        print(f"   ğŸ§¹ ê³µê°„ í™•ë³´: {len(delete_ids)}ê°œ ì‚­ì œ ì™„ë£Œ.")
    
    # [ìˆ˜ì •] ë­í‚¹ ì—…ë°ì´íŠ¸(_update_rankings) í˜¸ì¶œ ì œê±°

# _update_rankings í•¨ìˆ˜ ì‚­ì œë¨

def archive_top_articles():
    """
    ì ìˆ˜(Score) 7.0 ì´ìƒì¸ ê¸°ì‚¬ ë¬´ì¡°ê±´ ì•„ì¹´ì´ë¹™
    [ìˆ˜ì •] rank ê´€ë ¨ ì½”ë“œ ì™„ì „íˆ ì œê±°
    """
    print("ğŸ—„ï¸ ê³ ë“ì (7.0+) ê¸°ì‚¬ ì•„ì¹´ì´ë¹™ ì²´í¬...")
    
    try:
        res = supabase.table("live_news")\
            .select("*")\
            .gte("score", 7.0)\
            .execute()
        
        high_score_articles = res.data
        
        if high_score_articles:
            archive_data = []
            for art in high_score_articles:
                archive_data.append({
                    "created_at": art['created_at'],
                    "category": art['category'],
                    "title": art['title'],
                    "summary": art['summary'],
                    "image_url": art['image_url'],
                    "original_link": art['link'], 
                    "score": art['score']
                    # rank í•„ë“œ ì™„ì „íˆ ì‚­ì œë¨
                })
            
            supabase.table("search_archive").upsert(archive_data, on_conflict="original_link").execute()
            print(f"   ğŸ’¾ ì´ {len(archive_data)}ê°œì˜ ê³ ë“ì  ê¸°ì‚¬(7.0+) ì•„ì¹´ì´ë¸Œ ì €ì¥ ì™„ë£Œ.")
        else:
            print("   â„¹ï¸ ì €ì¥í•  ê³ ë“ì (7.0 ì´ìƒ) ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"   âš ï¸ ì•„ì¹´ì´ë¸Œ ì €ì¥ ì‹¤íŒ¨: {e}")

def update_keywords_db(keywords):
    if not keywords: return
    try:
        supabase.table("trending_keywords").delete().neq("id", 0).execute()
    except: pass 
    
    insert_data = []
    for i, item in enumerate(keywords):
        insert_data.append({
            "keyword": item.get('keyword'),
            "count": item.get('count', 0),
            "rank": item.get('rank', i + 1), # í‚¤ì›Œë“œ ë­í‚¹ì€ ìœ ì§€ (1~10ìœ„)
            "updated_at": datetime.now().isoformat()
        })
    
    if insert_data:
        try:
            supabase.table("trending_keywords").insert(insert_data).execute()
            print("   âœ… í‚¤ì›Œë“œ ë­í‚¹ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        except: pass

def get_recent_titles(limit=100):
    res = supabase.table("live_news").select("title").order("created_at", desc=True).limit(limit).execute()
    return [item['title'] for item in res.data]
