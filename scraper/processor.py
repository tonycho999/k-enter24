# scraper/processor.py
import database, gemini_api, naver_api
from datetime import datetime

def run_category_process(category):
    print(f"\nğŸš€ [Autonomous Processing] Category: {category}")

    # 1. AIì—ê²Œ ì§ì ‘ êµ¬ê¸€ ê²€ìƒ‰ì„ í†µí•œ íŠ¸ë Œë“œ ë¶„ì„ ìš”ì²­
    rank_rule = "SONG titles and ARTIST names" if category == "K-Pop" else \
                "DRAMA titles and ACTOR names" if category == "K-Drama" else \
                "MOVIE titles and ACTOR names" if category == "K-Movie" else \
                "TV SHOW titles and CAST names" if category == "K-Entertain" else \
                "Hot PLACES and TRADITIONAL culture (Exclude Celebrities)"

    prompt = f"""
    Search Google for the latest {category} trends in Korea as of today.
    1. Identify the TOP 10 trending {rank_rule.split(' and ')[0]}.
    2. Provide the ENGLISH display title and the original KOREAN title for each.
    3. Pick the #1 trending SUBJECT (person or place) in KOREAN for a deep-dive search.

    Return results strictly in JSON:
    {{
      "rankings": [
        {{"rank": 1, "display_title_en": "English Name", "search_keyword_kr": "í•œêµ­ì–´ ì›ë³¸", "meta": "Reason", "score": 95}}
      ],
      "top_person_kr": "í•œêµ­ì–´ ê²€ìƒ‰ì–´(ì¬ê²€ìƒ‰ìš©)",
      "top_subject_en": "English Name(DBìš©)"
    }}
    """
    
    print(f"   1ï¸âƒ£ AI is searching Google for {category} trends...")
    rank_res = gemini_api.ask_gemini(prompt)
    if not rank_res: return

    # 2. ë­í‚¹ ì €ì¥ (ì‘í’ˆ ì œëª© ì¤‘ì‹¬)
    database.save_rankings_to_db(rank_res.get("rankings", []))

    # 3. ì¿¨íƒ€ì„ ì²´í¬ (ì¸ë¬¼/ì¥ì†Œ ì¤‘ì‹¬)
    target_kr = rank_res.get("top_person_kr")
    target_en = rank_res.get("top_subject_en")

    if database.is_keyword_used_recently(category, target_en, hours=4):
        print(f"   ğŸ•’ '{target_en}' is on cooldown.")
        return

    # 4. ê¸°ì‚¬ ì‘ì„±ì„ ìœ„í•œ ì‹¬ì¸µ ê²€ìƒ‰ (ì—¬ê¸°ì„œë§Œ ë„¤ì´ë²„ API ì‚¬ìš©)
    # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë§Œìœ¼ë¡œëŠ” ë³¸ë¬¸ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì •í™•í•œ ê¸°ì‚¬ ë³¸ë¬¸ì€ ë„¤ì´ë²„ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    print(f"   2ï¸âƒ£ Deep searching Naver for article details of '{target_kr}'...")
    deep_items = naver_api.search_news_api(target_kr, display=5, sort='date')
    
    full_texts = []
    main_image = ""
    for item in deep_items:
        crawled = naver_api.crawl_article(item['link'])
        if crawled['text'] and len(crawled['text']) > 300:
            full_texts.append(crawled['text'])
            if not main_image and crawled['image'].startswith("https://"):
                main_image = crawled['image']
        if len(full_texts) >= 3: break

    if not full_texts: return

    # 5. ë² í…Œë‘ ê¸°ì ìŠ¤íƒ€ì¼ë¡œ ì˜ì–´ ê¸°ì‚¬ ì‘ì„±
    article_prompt = f"You are a veteran journalist. Write a professional English news report about {target_en} based on these sources: {str(full_texts)[:5000]}. Return JSON: {{'title': '...', 'content': '...'}}"
    news_res = gemini_api.ask_gemini(article_prompt)

    if news_res:
        news_item = {
            "category": category, "keyword": target_en,
            "title": news_res.get("title"), "summary": news_res.get("content"),
            "image_url": main_image, "score": 100, "created_at": datetime.now().isoformat(), "likes": 0
        }
        database.save_news_to_live([news_item])
        database.save_news_to_archive([news_item])
        print(f"   ğŸ‰ SUCCESS: '{target_en}' published via Google Search Grounding.")
