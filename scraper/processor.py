# scraper/processor.py
import time
from datetime import datetime
import config
import naver_api
import gemini_api
import database

def run_category_process(category):
    print(f"\nğŸš€ [Processing] Category: {category}")

    # 1. [ê´‘ë²”ìœ„ íƒìƒ‰] APIë¡œ 100ê°œ ìˆ˜ì§‘
    keyword = config.SEARCH_KEYWORDS.get(category)
    raw_items = naver_api.search_news_api(keyword, display=100)
    
    if not raw_items:
        print("   âš ï¸ No items found from Naver API.")
        return

    titles = "\n".join([f"- {item['title']}" for item in raw_items])

    # 2. [ë­í‚¹ ì„ ì •] AIì—ê²Œ Top 10 í‚¤ì›Œë“œ/ìˆœìœ„ ì¶”ì¶œ ìš”ì²­
    rank_prompt = f"""
    [Task]
    Analyze these news titles about {category}.
    1. Identify Top 10 trending keywords (Person, Group, Work).
    2. Provide a short meta info for each.
    
    [Output JSON Format]
    {{
        "rankings": [
            {{ "rank": 1, "keyword": "Name", "meta": "Reason", "score": 95 }}
        ]
    }}
    """
    
    rank_res = gemini_api.ask_gemini(rank_prompt)
    if not rank_res: return

    rankings = rank_res.get("rankings", [])[:10]
    
    # 2-1. ë­í‚¹ DB ì €ì¥ (ë³´ì—¬ì£¼ê¸°ìš©)
    db_rankings = []
    for item in rankings:
        db_rankings.append({
            "category": category,
            "rank": item.get("rank"),
            "title": item.get("keyword"),
            "meta_info": item.get("meta", ""),
            "score": item.get("score", 0),
            "updated_at": datetime.now().isoformat()
        })
    database.save_rankings_to_db(db_rankings)

    # 3. [íƒ€ê²Ÿ ì„ ì •] ë„ë°° ë°©ì§€ (ì¿¨íƒ€ì„ 4ì‹œê°„)
    target_keyword = ""
    print("   ğŸ›¡ï¸ Checking cooldowns (4 hours)...")
    
    for item in rankings:
        candidate = item.get("keyword")
        # DB ì²´í¬: ìµœê·¼ì— ì¼ë‹ˆ?
        if database.is_keyword_used_recently(category, candidate, hours=4):
            print(f"      âŒ Skip '{candidate}' (Cooldown active).")
        else:
            print(f"      âœ… Selected target: '{candidate}'!")
            target_keyword = candidate
            break
    
    # ë§Œì•½ 10ê°œ ë‹¤ ì¿¨íƒ€ì„ ê±¸ë ¸ìœ¼ë©´? (ë“œë¬¼ì§€ë§Œ) -> ê·¸ëƒ¥ 1ìœ„ ê°•ì œ ì„ íƒ
    if not target_keyword and rankings:
        target_keyword = rankings[0].get("keyword")
        print(f"      âš ï¸ All candidates on cooldown. Forced select: '{target_keyword}'")

    if not target_keyword: return

    # 4. [ì •ë°€ ìˆ˜ì§‘] í™•ì •ëœ í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ 3ê°œ ìˆ˜ì§‘
    print(f"   ğŸ¯ Deep diving into: {target_keyword}")
    target_items = naver_api.search_news_api(target_keyword, display=3)
    
    full_texts = []
    target_link = ""
    target_image = ""

    for item in target_items:
        link = item['link']
        crawled = naver_api.crawl_article(link)
        
        if crawled['text']:
            full_texts.append(crawled['text'])
            if not target_image: target_image = crawled['image']
            if not target_link: target_link = link
        else:
            full_texts.append(item['description'])
            if not target_link: target_link = link

    if not full_texts: return

    # 5. [ìš”ì•½] AI ê¸°ì‚¬ ì‘ì„±
    summary_prompt = f"""
    [Articles about '{target_keyword}']
    {str(full_texts)[:6000]}

    [Task]
    Write a high-quality news summary in Korean.
    [Output JSON]
    {{ "title": "Catchy Title", "summary": "Detailed summary (3-5 sentences)" }}
    """
    
    sum_res = gemini_api.ask_gemini(summary_prompt)
    
    if sum_res:
        news_item = {
            "category": category,
            "keyword": target_keyword,
            "title": sum_res.get("title", f"{target_keyword} ì´ìŠˆ"),
            "summary": sum_res.get("summary", ""),
            "link": target_link,
            "image_url": target_image,
            "score": 100, # 1ìœ„ ì„ ì •ëœ ê±´ì´ë¯€ë¡œ ë§Œì 
            "created_at": datetime.now().isoformat(),
            "likes": 0
        }
        
        # 6. [ì´ì›í™” ì €ì¥] Live(ì „ì‹œìš©) + Archive(ë³´ê´€ìš©)
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬
        database.save_news_to_live([news_item])     # Liveì— ì €ì¥
        database.save_news_to_archive([news_item])  # Archiveì— ì €ì¥
        
        # 7. [ì²­ì†Œ] Live í…Œì´ë¸”ë§Œ 30ê°œ ìœ ì§€
        database.cleanup_old_data(category, config.MAX_ITEMS_PER_CATEGORY)
